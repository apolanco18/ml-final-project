{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apolanco18/ml-final-project/blob/master/ml_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVpNhTCBdF-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zklbm4bVzIAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import xgboost as xgb\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "\n",
        "class ParameterTuning(object):\n",
        "  init = None\n",
        "\n",
        "\n",
        "  def __init__(self,init):\n",
        "    self.init = init\n",
        "\n",
        "  \n",
        "  def model_metrics(self,totalTime,yTrain,yHatTrain,yTest,yHat):\n",
        "    totalTime = time.time() - totalTime\n",
        "\n",
        "    trainAcc = accuracy_score(yTrain, yHatTrain)\n",
        "    testAcc = accuracy_score(yTest, yHat)\n",
        "\n",
        "    f1 = f1_score(yTest, yHat, average='macro')\n",
        "    matrix = metrics.confusion_matrix(yTest, yHat)\n",
        "\n",
        "    stats = {}\n",
        "    stats['train accuracy'] = trainAcc\n",
        "    stats['test accuracy'] = testAcc\n",
        "    stats['macro f1 score'] = f1\n",
        "    stats['time taken'] = totalTime\n",
        "\n",
        "    return stats,matrix\n",
        "\n",
        "  def print_stats(self,matrix,stats):\n",
        "    # print(matrix)\n",
        "    print('Train Accuracy {}'.format(stats['train accuracy']))\n",
        "    print('Test Accuracy {}'.format(stats['test accuracy']))\n",
        "    print('Macro F1 Score {}'.format(stats['macro f1 score']))\n",
        "    print('Time Taken {}'.format(stats['time taken']))    \n",
        "  \n",
        "\n",
        "  def testModel(self,model,xTrain, xTest, yTrain, yTest): #use this to get data on an individual model\n",
        "\n",
        "    yTrain = np.reshape(yTrain, (len(yTrain),1))\n",
        "    \n",
        "    \n",
        "    model.fit(xTrain, yTrain)\n",
        "    yHatTrain = model.predict(xTrain)\n",
        "\n",
        "    totalTime = time.time()\n",
        "    yHat = model.predict(xTest)\n",
        "\n",
        "\n",
        "    ##-----------Metrics------------------##\n",
        "\n",
        "    stats,matrix = self.model_metrics(totalTime,yTrain,yHatTrain,yTest,yHat)\n",
        "\n",
        "    return stats,matrix\n",
        "\n",
        "  def knn(self,nn,xTrain,yTrain,xTest,yTest):\n",
        "    knn = KNeighborsClassifier(n_neighbors = nn)\n",
        "\n",
        "    stats,matrix = self.testModel(knn,xTrain,xTest,yTrain,yTest)\n",
        "\n",
        "    print(\"KNN: {}\".format(nn))\n",
        "    self.print_stats(matrix,stats)\n",
        "\n",
        "    return nn,stats\n",
        "  \n",
        "  def write_knn_results(self,type,data):\n",
        "\n",
        "    filepath = '/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/Results/KNN/knn_data_' + type + '.csv'\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "      fileWriter = csv.writer(f,delimiter=',',quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n",
        "      fileWriter.writerow(['nn','train accuracy','test accuracy', 'macro f1 score','time taken'])\n",
        "      for i in range(len(data)):\n",
        "        fileWriter.writerow([data[i]['nn'],data[i]['stats']['train accuracy'],data[i]['stats']['test accuracy'],data[i]['stats']['macro f1 score'],data[i]['stats']['time taken']])\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "  def param_tuning_knn(self,type,xTrain,yTrain,xTest,yTest):\n",
        "\n",
        "    listOfData = []\n",
        "\n",
        "    for i in range(1,16):\n",
        "      nn,stats = self.knn(i,xTrain,yTrain,xTest,yTest)\n",
        "\n",
        "      finalStats = {}\n",
        "      finalStats['nn'] = nn\n",
        "      finalStats['stats'] = stats\n",
        "      listOfData.append(finalStats)\n",
        "\n",
        "\n",
        "    self.write_knn_results(type,listOfData)\n",
        "\n",
        "  \n",
        "  def logReg(self,penalty,xTrain,yTrain,xTest,yTest):\n",
        "    logReg = LogisticRegression(penalty = penalty, solver = 'lbfgs', multi_class = 'auto')\n",
        "\n",
        "    stats,matrix = self.testModel(logReg,xTrain,xTest,yTrain,yTest)\n",
        "\n",
        "    print(\"Logistic Regression - Penalty: {}\".format(penalty))\n",
        "    self.print_stats(matrix,stats)\n",
        "\n",
        "    return penalty,stats\n",
        "\n",
        "  def write_logReg_results(self,type,data):\n",
        "\n",
        "    filepath = '/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/Results/Logistic Regression/logReg_data_' + type + '.csv'\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "      fileWriter = csv.writer(f,delimiter=',',quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n",
        "      fileWriter.writerow(['penalty','train accuracy','test accuracy', 'macro f1 score','time taken'])\n",
        "      for i in range(len(data)):\n",
        "        fileWriter.writerow([data[i]['penalty'],data[i]['stats']['train accuracy'],data[i]['stats']['test accuracy'],data[i]['stats']['macro f1 score'],data[i]['stats']['time taken']])\n",
        "    f.close()\n",
        "\n",
        "  def param_tuning_logReg(self,type,xTrain,yTrain,xTest,yTest):\n",
        "\n",
        "    penalties = ['none','l2']\n",
        "\n",
        "    listOfData = []\n",
        "\n",
        "    for penalty in penalties:\n",
        "      penalty,stats = self.logReg(penalty,xTrain,yTrain,xTest,yTest)\n",
        "\n",
        "      finalStats = {}\n",
        "      finalStats['penalty'] = penalty\n",
        "      finalStats['stats'] = stats\n",
        "      listOfData.append(finalStats)\n",
        "\n",
        "    self.write_logReg_results(type,listOfData)\n",
        "\n",
        "  def dt(self,criterion,maxDepth,minLeafSample,xTrain,yTrain,xTest,yTest):\n",
        "    dt = DecisionTreeClassifier(criterion = criterion,max_depth = maxDepth, min_samples_leaf = minLeafSample)\n",
        "\n",
        "    stats,matrix = self.testModel(dt,xTrain,yTrain,xTest,yTest)\n",
        "\n",
        "    # self.print_stats(matrix,stats)\n",
        "\n",
        "    return criterion,maxDepth,minLeafSample,stats\n",
        "\n",
        "  def write_dt_results(self,type,data):\n",
        "\n",
        "    filepath = '/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/Results/Decision Tree/dt_data_' + type + '.csv'\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "      fileWriter = csv.writer(f,delimiter=',',quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n",
        "      fileWriter.writerow(['criterion','max depth','min leaf samples','train accuracy','test accuracy', 'macro f1 score','time taken'])\n",
        "      for i in range(len(data)):\n",
        "        fileWriter.writerow([data[i]['criterion'],data[i]['max depth'],data[i]['min leaf samples'],data[i]['stats']['train accuracy'],data[i]['stats']['test accuracy'],data[i]['stats']['macro f1 score'],data[i]['stats']['time taken']])\n",
        "    f.close()\n",
        "\n",
        "  def param_tuning_dt(self,type,xTrain,yTrain,xTest,yTest):\n",
        "\n",
        "    listOfData = []\n",
        "\n",
        "    for g in ['gini']:\n",
        "      for maxDepth in range(1,26):\n",
        "        for minLeafSample in range(20,101,20):\n",
        "          criterion,maxDepth,minLeafSample,stats = self.dt(g,maxDepth,minLeafSample,xTrain,yTrain,xTest,yTest)\n",
        "\n",
        "          finalStats = {}\n",
        "          finalStats['criterion'] = criterion\n",
        "          finalStats['max depth'] = maxDepth\n",
        "          finalStats['min leaf samples'] = minLeafSample\n",
        "          finalStats['stats'] = stats\n",
        "          listOfData.append(finalStats)\n",
        "\n",
        "    self.write_dt_results(type,listOfData)\n",
        "\n",
        "  \n",
        "\n",
        "  def naive(self,xTrain,yTrain,xTest,yTest):\n",
        "    nv = GaussianNB()\n",
        "\n",
        "    stats,matrix = self.testModel(nv,xTrain,xTest,yTrain,yTest)\n",
        "\n",
        "    # self.print_stats(matrix,stats)\n",
        "\n",
        "    return stats\n",
        "\n",
        "  def write_naive_results(self,type,data):\n",
        "\n",
        "    filepath = '/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/Results/Naive Bayes/naive_data_' + type + '.csv'\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "      fileWriter = csv.writer(f,delimiter=',',quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n",
        "      fileWriter.writerow(['train accuracy','test accuracy', 'macro f1 score','time taken'])\n",
        "      for i in range(len(data)):\n",
        "        fileWriter.writerow([data[i]['stats']['train accuracy'],data[i]['stats']['test accuracy'],data[i]['stats']['macro f1 score'],data[i]['stats']['time taken']])\n",
        "    f.close()\n",
        "\n",
        "\n",
        "  def param_tuning_naive(self,type,xTrain,yTrain,xTest,yTest):\n",
        "    listOfData = []\n",
        "\n",
        "\n",
        "    stats = self.naive(xTrain,yTrain,xTest,yTest)\n",
        "\n",
        "    finalStats = {}\n",
        "    finalStats['stats'] = stats\n",
        "    listOfData.append(finalStats)\n",
        "\n",
        "    self.write_naive_results(type,listOfData)\n",
        "\n",
        "  \n",
        "  def write_neural_results(self,type,data):\n",
        "\n",
        "    filepath = '/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/Results/Neural Network/neuralN_data_' + type + '.csv'\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "      fileWriter = csv.writer(f,delimiter=',',quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n",
        "      fileWriter.writerow(['train accuracy','test accuracy', 'macro f1 score','time taken'])\n",
        "      for i in range(len(data)):\n",
        "        fileWriter.writerow([data[i]['stats']['train accuracy'],data[i]['stats']['test accuracy'],data[i]['stats']['macro f1 score'],data[i]['stats']['time taken']])\n",
        "    f.close()\n",
        "\n",
        "  \n",
        "  def rf(self,numTrees,maxDepth,xTrain,yTrain,xTest,yTest):\n",
        "    rf = RandomForestClassifier(n_estimators= numTrees, max_depth= maxDepth)\n",
        "\n",
        "    stats,matrix = self.testModel(rf,xTrain, xTest, yTrain, yTest)\n",
        "\n",
        "    return numTrees, maxDepth, stats\n",
        "  \n",
        "  def write_rf_results(self,type,data):\n",
        "\n",
        "    filepath = '/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/Results/Random Forest/rf_data_' + type + '.csv'\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "      fileWriter = csv.writer(f,delimiter=',',quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n",
        "      fileWriter.writerow(['number of trees','max depth','train accuracy','test accuracy', 'macro f1 score','time taken'])\n",
        "      for i in range(len(data)):\n",
        "        fileWriter.writerow([data[i]['number of trees'],data[i]['max depth'],data[i]['stats']['train accuracy'],data[i]['stats']['test accuracy'],data[i]['stats']['macro f1 score'],data[i]['stats']['time taken']])\n",
        "    f.close()  \n",
        "\n",
        "  def param_tuning_rf(self,type,xTrain,yTrain,xTest,yTest):\n",
        "\n",
        "    listOfData = []\n",
        "\n",
        "\n",
        "    for i in range(100,201,25):\n",
        "      for maxDepth in range(2,26,2):\n",
        "          numTrees,maxDepth,stats = self.rf(i,maxDepth,xTrain,yTrain,xTest,yTest)\n",
        "\n",
        "          finalStats = {}\n",
        "          finalStats['number of trees'] = numTrees\n",
        "          finalStats['max depth'] = maxDepth\n",
        "          finalStats['stats'] = stats\n",
        "          listOfData.append(finalStats)\n",
        "\n",
        "    self.write_rf_results(type,listOfData)\n",
        "\n",
        "  \n",
        "  def xgb(self,eta,maxDepth,esti,xTrain,yTrain,xTest,yTest):\n",
        "    booster = xgb.XGBClassifier(max_depth = maxDepth,eta = eta, n_estimators = esti)\n",
        "\n",
        "    stats,matrix = self.testModel(booster,xTrain, xTest, yTrain, yTest)\n",
        "\n",
        "    return eta,maxDepth,esti,stats\n",
        "\n",
        "  def write_xgb_results(self,type,data):\n",
        "\n",
        "    filepath = '/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/Results/XGB Boost/xgb_data_' + type + '.csv'\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "      fileWriter = csv.writer(f,delimiter=',',quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n",
        "      fileWriter.writerow(['eta','max depth','number of estimators','train accuracy','test accuracy', 'macro f1 score','time taken'])\n",
        "      for i in range(len(data)):\n",
        "        fileWriter.writerow([data[i]['eta'],data[i]['max depth'],data[i]['number of estimators'],data[i]['stats']['train accuracy'],data[i]['stats']['test accuracy'],data[i]['stats']['macro f1 score'],data[i]['stats']['time taken']])\n",
        "    f.close()  \n",
        "\n",
        "  def param_tuning_xgb(self,type,xTrain,yTrain,xTest,yTest):\n",
        "\n",
        "    listOfData = []\n",
        "\n",
        "\n",
        "    for eta in [.1,.2,.3,.4]:\n",
        "      for maxDepth in range(2,8,2):\n",
        "          for esti in range(5,8,1):\n",
        "            eta,maxDepth,esti,stats = self.xgb(eta,maxDepth,esti,xTrain,yTrain,xTest,yTest)\n",
        "\n",
        "            finalStats = {}\n",
        "            finalStats['eta'] = eta\n",
        "            finalStats['max depth'] = maxDepth\n",
        "            finalStats['number of estimators'] = esti\n",
        "            finalStats['stats'] = stats\n",
        "            listOfData.append(finalStats)\n",
        "\n",
        "    self.write_xgb_results(type,listOfData)   \n",
        "          \n",
        "\n",
        "  def test_all_models(self,val,type,xTrain,yTrain,xTest,yTest):\n",
        "    if val == 1:\n",
        "      self.param_tuning_knn(type,xTrain,yTrain,xTest,yTest)\n",
        "    elif val == 2:\n",
        "      self.param_tuning_logReg(type,xTrain,yTrain,xTest,yTest)\n",
        "    elif val == 3:\n",
        "      self.param_tuning_dt(type,xTrain,yTrain,xTest,yTest)\n",
        "    elif val == 4:\n",
        "      self.param_tuning_naive(type,xTrain,yTrain,xTest,yTest)\n",
        "    elif val == 5:\n",
        "      self.param_tuning_rf(type,xTrain,yTrain,xTest,yTest)\n",
        "    elif val == 6:\n",
        "      self.param_tuning_xgb(type,xTrain,yTrain,xTest,yTest)\n",
        "    \n",
        "      \n",
        "    \n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main file to run from the command line.\n",
        "    \"\"\"\n",
        "    # set up the program to take in arguments from the command line\n",
        "    np.random.seed(0)\n",
        "\n",
        "    drive.mount('/content/drive/')\n",
        "\n",
        "    xTrain = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTrain.npy', encoding = 'bytes')\n",
        "    xTest = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTest.npy', encoding = 'bytes')\n",
        "\n",
        "    xTrainScale = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTrainScale.npy', encoding = 'bytes')\n",
        "    xTestScale = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTestScale.npy', encoding = 'bytes')\n",
        "\n",
        "    xTrainFeatElim = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTrainFeatElim.npy', encoding = 'bytes')\n",
        "    xTestFeatElim = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTestFeatElim.npy', encoding = 'bytes')\n",
        "\n",
        "    xTrainPCA = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTrainPCA.npy', encoding = 'bytes')\n",
        "    xTestPCA = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTestPCA.npy', encoding = 'bytes')\n",
        "    \n",
        "\n",
        "    yTrain = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/yTrain.npy', encoding = 'bytes')\n",
        "    yTest = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/yTest.npy', encoding = 'bytes')\n",
        "\n",
        "\n",
        "    tuning = ParameterTuning('Start')\n",
        "\n",
        "\n",
        "    types = ['norm','scale','feat-elim','pca']\n",
        "    data = [[xTrain,xTest],[xTrainScale,xTestScale],[xTrainFeatElim,xTestFeatElim],[xTrainPCA,xTestPCA]]\n",
        "\n",
        "    \n",
        "\n",
        "    '''\n",
        "      KNN Model Parameter Tuning\n",
        "\n",
        "    '''\n",
        "    # for i in range(2,3):\n",
        "    #   tuning.test_all_models(1,types[i],data[i][0],yTrain,data[i][1],yTest)\n",
        "\n",
        "    '''\n",
        "      Logistic Regression Model Parameter Tuning\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    # for i in range(0,4):\n",
        "    #   tuning.test_all_models(2,types[i],data[i][0],yTrain,data[i][1],yTest)\n",
        "\n",
        "    '''\n",
        "      Decision Tree Model Parameter Tuning\n",
        "    \n",
        "    '''\n",
        "    # for i in range(0,4):\n",
        "    #   tuning.test_all_models(3,types[i],data[i][0],yTrain,data[i][1],yTest)\n",
        "\n",
        "    '''\n",
        "      Naives Bayes Model Parameter Tuning\n",
        "    \n",
        "    '''\n",
        "    # for i in range(0,4):\n",
        "    #   tuning.test_all_models(4,types[i],data[i][0],yTrain,data[i][1],yTest)\n",
        "\n",
        "    '''\n",
        "      XGB Booster Model Parameter Tuning\n",
        "    \n",
        "    '''\n",
        "\n",
        "    # for i in range(0,4):\n",
        "    #   tuning.test_all_models(6,types[i],data[i][0],yTrain,data[i][1],yTest)\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "      Neural Network Model Parameter Tuning\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    # for i in range(0,4):\n",
        "    #   neuralN = MLPClassifier()\n",
        "\n",
        "    #   stats,matrix = tuning.testModel(neuralN,data[i][0], data[i][1], yTrain, yTest)\n",
        "      \n",
        "    #   temp = []\n",
        "    #   temp2 = {}\n",
        "    #   temp2['stats'] = stats\n",
        "    #   temp.append(temp2)\n",
        "    #   tuning.write_neural_results(types[i],temp)\n",
        "\n",
        "    '''\n",
        "      Random Forest Model Parameter Tuning\n",
        "    \n",
        "    '''\n",
        "\n",
        "    # for i in range(0,4):\n",
        "    #   tuning.test_all_models(5,types[i],data[i][0],yTrain,data[i][1],yTest)\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eBp_pz3ZKfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "auth.authenticate_user()\n",
        "import xgboost as xgb\n",
        "\n",
        "class Preprocess(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    print(\"hello world\")\n",
        "\n",
        "\n",
        "  # Scales the data so the each feature can be compared to each other\n",
        "  def scale_data(self, xFeat):\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    return scaler.fit_transform(xFeat)\n",
        "\n",
        "  \n",
        "  # Calculate Pearson Correlation Matrix \n",
        "  def pearsonMatrix(self,xFeat):\n",
        "      featCol = []\n",
        "\n",
        "      for i in range(len(xFeat[0])):\n",
        "        featCol.append(xFeat[:,i])\n",
        "      \n",
        "      pearsonMatrix = np.corrcoef(featCol)\n",
        "\n",
        "      return pearsonMatrix\n",
        "\n",
        "  \n",
        "  # Graphs the heat map\n",
        "  def graphMatrix(self,matrix):\n",
        "    sns.set()\n",
        "\n",
        "    ax = sns.heatmap(matrix)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  # Follows the idea of Feature elimination where a pearson correlation matrix is calculated and then the highly correlated features are dropped reduce the number of features\n",
        "  def feature_elimination(self,xFeat):\n",
        "\n",
        "    matrix = self.pearsonMatrix(xFeat)\n",
        "    \n",
        "    deleteColBool = np.full( (1, len(xFeat[0])), True)\n",
        "\n",
        "    deleteColBool = deleteColBool[0]\n",
        "\n",
        "    for i in range(len(xFeat[0])):\n",
        "      for t in range(i+1, len(xFeat[0])):\n",
        "        if (abs(matrix[i,t]) >= .75):\n",
        "          deleteColBool[t] = False\n",
        "\n",
        "    deleteColIndex = []\n",
        "\n",
        "    for i in range(len(deleteColBool)):\n",
        "        if (deleteColBool[i] == False):\n",
        "            deleteColIndex.append(i)\n",
        "\n",
        "    xFeat = np.delete(xFeat,deleteColIndex,axis = 1)\n",
        "\n",
        "\n",
        "    return xFeat\n",
        "\n",
        "\n",
        "  # Finds the PCA whcih gives 95% and then transfroms it\n",
        "  def pca(self,xFeatTrain,xFeatTest):\n",
        "    model = PCA(.95)\n",
        "\n",
        "    model.fit(xFeatTrain)\n",
        "\n",
        "    self.graph_variance_pca(model)\n",
        "\n",
        "    xTrainReduce = model.transform(xFeatTrain)\n",
        "    xTestReduce = model.transform(xFeatTest)\n",
        "\n",
        "    return xTrainReduce,xTestReduce\n",
        "\n",
        "  # Graphs the number of components vs explained variance for PCS\n",
        "  def graph_variance_pca(self,model):\n",
        "    plt.plot(np.cumsum(model.explained_variance_ratio_))\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "  \n",
        "  # Graphing the first 2 principal components \n",
        "  def graph_pca_components(self,xFeat,yFeat):\n",
        "    xAxis = xFeat[:,0]\n",
        "    yAxis = xFeat[:,1]\n",
        "\n",
        "    labels = yFeat \n",
        "\n",
        "    colorDict = {0:'red',1:'green',2:'black',3:'yellow',4:'#1004fb',5:'cyan',6:'magenta',7:'green',8:'#e1341e',9:'#3820df',10:'#d52ad1'}\n",
        "\n",
        "    label = {0:'Benign',1:'Ack',2:'Combo',3:'Junk',4:'Scan 1',5:'Scan',6:'Syn 1',7:'Tcp',8:'Udp 1',9:'Udp',10:'Udp Plain 1'}\n",
        "\n",
        "    marker = {0:'*',1:'o',2:'.',3:'v',4:'1',5:'8',6:'s',7:'p',8:'P',9:'h',10:'x'}\n",
        "\n",
        "    alpha = {0:.3,1:.5,2:.7,3:.9,4:1.1,5:1.3,6:1.5,7:1.7,8:1.9,9:2.1,10:2.3}\n",
        "\n",
        "    fig,ax = plt.subplots(figsize = (7,5))\n",
        "    fig.patch.set_facecolor('white')\n",
        "\n",
        "    for l in np.unique(labels):\n",
        "      if l == 4:\n",
        "        break\n",
        "      ix = np.where(labels == l) \n",
        "      ax.scatter(xAxis[ix],yAxis[ix], c = colorDict[l],s = 40, label = label[l],marker = marker[l], alpha = .5)\n",
        "\n",
        "    plt.xlabel(\"First Principal Component\",fontsize=14)\n",
        "    plt.ylabel(\"Second Principal Component\",fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  # write the preprocessed data to drive\n",
        "  def write_data_file(self,name,data):\n",
        "    filepath = '/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/' + name + '.npy'\n",
        "    with open(filepath, 'w') as f:\n",
        "      np.save(f.name,data)\n",
        "      f.close()\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  drive.mount('/content/drive/')\n",
        "\n",
        "  xTrain = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTrain.npy', encoding = 'bytes')\n",
        "  xTest = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/xTest.npy', encoding = 'bytes')\n",
        "  yTrain = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/yTrain.npy', encoding = 'bytes')\n",
        "  yTest = np.load('/content/drive/My Drive/19-20SchoolYear/CS334/Machine Learning Project/yTest.npy', encoding = 'bytes')\n",
        "\n",
        "\n",
        "  preprocess = Preprocess()\n",
        "\n",
        "\n",
        "  xTrainScale = preprocess.scale_data(xTrain)\n",
        "  xTestScale = preprocess.scale_data(xTest)\n",
        "\n",
        "  # Training and Test data features reduce using pearson correlation matrix for feature elimination\n",
        "  xTrainFeatElim = preprocess.feature_elimination(xTrainScale)\n",
        "  xTestFeatElim = preprocess.feature_elimination(xTestScale)\n",
        "\n",
        "\n",
        "  # preprocess.graphMatrix(preprocess.pearsonMatrix(xTrainScale))\n",
        "  # preprocess.graphMatrix(preprocess.pearsonMatrix(xTestScale))\n",
        "\n",
        "  # preprocess.graphMatrix(preprocess.pearsonMatrix(xTrainFeatElim))\n",
        "  # preprocess.graphMatrix(preprocess.pearsonMatrix(xTestFeatElim))\n",
        "\n",
        "  xTrainPCA,xTestPCA = preprocess.pca(xTrainScale,xTestScale)\n",
        "\n",
        "  preprocess.write_data_file('xTrainScale',xTrainScale)\n",
        "  preprocess.write_data_file('xTestScale',xTestScale)\n",
        "\n",
        "  preprocess.write_data_file('xTrainFeatElim',xTrainFeatElim)\n",
        "  preprocess.write_data_file('xTestFeatElim',xTestFeatElim)\n",
        "  \n",
        "  preprocess.write_data_file('xTrainPCA',xTrainPCA)\n",
        "  preprocess.write_data_file('xTestPCA',xTestPCA)\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  # preprocess.graph_pca_components(xTrainPCA,yTrain)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}